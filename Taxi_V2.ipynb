{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEc8c4sSlx6H"
      },
      "source": [
        "### OpenAI Taxi-V2 Player via Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Dm3irjblx6K"
      },
      "source": [
        "#### Load Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MksIPrdlx6L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c9dc1a5d-23b5-458a-847b-bda1e77bd697"
      },
      "source": [
        "# IMPORT MODULES\n",
        "# Import Numpy, Gym etc\n",
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "print('Import Modules')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Import Modules\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcNVP8kXlx6T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "33556784-90cc-4bab-cf1b-35fb8a29cf87"
      },
      "source": [
        "# CREATE ENVIRONMENT\n",
        "# Load Taxi-V2 Environment\n",
        "# In this Environment the Yellow Square represents the Taxi, the (“|”) represents a Wall, the Blue Letter represents the Pick-Up Location, and the Purple letter is the Drop-Off Location. The Taxi will turn Green when it has a Passenger Aboard.\n",
        "\n",
        "Env = gym.make(\"Taxi-v3\").env\n",
        "\n",
        "state = Env.encode(1, 1,3, 2)\n",
        "print(\"State:\", state)\n",
        "\n",
        "Env.s = state\n",
        "\n",
        "\n",
        "Env.render()\n",
        "\n",
        "print('Load Taxi-V3 Environment')\n",
        "print('In this Environment the Yellow Square represents the Taxi, the (“|”) represents a Wall.')\n",
        "print('The Blue Letter represents the Pick-Up Location, and the Purple letter is the Drop-Off Location.')\n",
        "print('The Taxi will turn Green when it has a Passenger Aboard.')\n",
        "print('The Environment gives a -1 Reward for each Step in order for the Agent to try and find the quickest solution.')\n",
        "print('The Environment gives a -10 Reward if Agent incorrectly Picks Up or Drops Off a Passenger.')\n",
        "print('The Environment gives a 20 Reward on Success')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "State: 134\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| :\u001b[43m \u001b[0m| : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "\n",
            "Load Taxi-V3 Environment\n",
            "In this Environment the Yellow Square represents the Taxi, the (“|”) represents a Wall.\n",
            "The Blue Letter represents the Pick-Up Location, and the Purple letter is the Drop-Off Location.\n",
            "The Taxi will turn Green when it has a Passenger Aboard.\n",
            "The Environment gives a -1 Reward for each Step in order for the Agent to try and find the quickest solution.\n",
            "The Environment gives a -10 Reward if Agent incorrectly Picks Up or Drops Off a Passenger.\n",
            "The Environment gives a 20 Reward on Success\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGjdZ_m6lx6X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "86f6d229-793b-4d6c-f78a-3c276e4c7a64"
      },
      "source": [
        "# LOAD ENVIRONMENT\n",
        "# Explore Environment\n",
        "ActionSize=Env.action_space.n\n",
        "print(\"Action Size \",ActionSize)\n",
        "StateSize=Env.observation_space.n\n",
        "print(\"State Size \",StateSize)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Action Size  6\n",
            "State Size  500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H13NFtkNlx6b"
      },
      "source": [
        "#### Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq7ZQ_6Elx6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "9b393b53-315c-4dab-a8c4-f8831269a893"
      },
      "source": [
        "# INITIALIZATION\n",
        "# Initialize Q-Table\n",
        "QTable=np.zeros((StateSize,ActionSize))\n",
        "print(QTable)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Cf4JhLolx6h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f2b29201-d777-45aa-fa6a-67dcee2b6938"
      },
      "source": [
        "# INITIALIZATION\n",
        "# Add Hyper-Parameters Episodes\n",
        "TotalEpisodes=50000        # Total Episodes\n",
        "TotalTestEpisodes=1        # Total Test Episodes\n",
        "MaxSteps=99                # Max Steps per Episode\n",
        "\n",
        "# Add Hyper-Parameters Bellman Equation\n",
        "LearningRate=0.7           # Learning Rate\n",
        "Gamma=0.618                # Discounting Rate\n",
        "\n",
        "# Add Exploration Parameters\n",
        "Epsilon=1.0                # Exploration rate\n",
        "MaxEpsilon=1.0             # Exploration probability at start\n",
        "MinEpsilon=0.01            # Minimum exploration probability\n",
        "DecayRate=0.01             # Exponential decay rate for exploration prob\n",
        "print('Add Hyper-Parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Add Hyper-Parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bpPif-4lx6m"
      },
      "source": [
        "#### Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7qWF_Z-lx6o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "f6c0dccf-c625-402f-bb70-5b5c3f44bece"
      },
      "source": [
        "# Q-LEARNING\n",
        "# Perform Learning for each Episode\n",
        "for Episode in range(TotalEpisodes):\n",
        "    # Reset the Environment\n",
        "    State=Env.reset()\n",
        "    State = Env.encode(1, 1, 3, 2)\n",
        "    Step=0\n",
        "    Done=False\n",
        "\n",
        "    # Perform Temporal Difference Learning for each Step\n",
        "    for Step in range(MaxSteps):\n",
        "        # Choose an Action (A) in Current World State (S)\n",
        "        # First Randomize a Number\n",
        "        ExploreExploitTradeoff=random.uniform(0,1)\n",
        "\n",
        "        # Check if this number is Greater than Epsilon, Then Exploitation (Take the Biggest Q-Value for this State)\n",
        "        if ExploreExploitTradeoff > Epsilon:\n",
        "            Action=np.argmax(QTable[State,:])\n",
        "\n",
        "        # Otherwise Exploration (Perform a Random Action)\n",
        "        else:\n",
        "            Action=Env.action_space.sample()\n",
        "\n",
        "        # Take the Action (A) and Observe the Outcome State(S') and Reward (R)\n",
        "        NewState,Reward,Done,Info=Env.step(Action)\n",
        "\n",
        "        # Update Q(S,A):= Q(S,A) + Learning Rate * [R(S,A) + Gamma * Max Q(S',A') - Q(S,A)]\n",
        "        QTable[State,Action]=QTable[State,Action]+LearningRate*(Reward+Gamma*np.max(QTable[NewState,:])-QTable[State,Action])\n",
        "\n",
        "        # Update State\n",
        "        State=NewState\n",
        "\n",
        "        # Check if Episode is Finished\n",
        "        if Done==True:\n",
        "            break\n",
        "\n",
        "    # Increment Episode\n",
        "    Episode+=1\n",
        "\n",
        "    # Reduce Epsilon (We Need Less and Less Exploration after each Episode)\n",
        "    Epsilon=MinEpsilon+(MaxEpsilon-MinEpsilon)*np.exp(-DecayRate*Episode)\n",
        "\n",
        "# Print Final Q-Table\n",
        "print(QTable)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0.           0.           0.           0.           0.\n",
            "    0.        ]\n",
            " [ -2.50421536  -2.43400544  -2.50421536  -2.43400544  -2.32039715\n",
            "  -11.43400544]\n",
            " [ -1.83910189  -1.35777005  -1.83910189  -1.35777005  -0.57891593\n",
            "  -10.35777005]\n",
            " ...\n",
            " [ -1.90700434  -1.57188499  -2.06537874  -1.35777005  -9.73869583\n",
            "  -10.49359028]\n",
            " [ -2.32039934  -2.15407486  -2.32050194  -2.13656497 -11.31748909\n",
            "  -11.31952473]\n",
            " [  6.01520692   2.68879548   5.35148302  11.36        -2.98928977\n",
            "   -3.3413632 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRcPQaBblx6t"
      },
      "source": [
        "#### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_khNQd6alx6v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "57db584a-3a60-4121-af97-a1d008d69545"
      },
      "source": [
        "# TEST\n",
        "# Test the Q-Learning via Playing\n",
        "\n",
        "State = Env.encode(1, 1, 3, 2)\n",
        "\n",
        "Rewards=[]\n",
        "\n",
        "# Run Player for each Episode\n",
        "for Episode in range(TotalTestEpisodes):\n",
        "    State=Env.reset()\n",
        "    State = Env.encode(1, 1, 3, 2)\n",
        "    Step=0\n",
        "    Done=False\n",
        "    TotalRewards=0\n",
        "    print(\"\")\n",
        "    print(\"Episode \",Episode)\n",
        "    for Step in range(MaxSteps):\n",
        "        Env.render()\n",
        "        # Take the Action (A) that have the Maximum Expected Future Reward given that State\n",
        "        Action=np.argmax(QTable[State,:])\n",
        "\n",
        "        # Update State\n",
        "        NewState,Reward,Done,Info=Env.step(Action)\n",
        "\n",
        "        # Update Rewards\n",
        "        TotalRewards +=Reward\n",
        "\n",
        "        # Check if Task Completed\n",
        "        if Done:\n",
        "            Rewards.append(TotalRewards)\n",
        "            print (\"Score \",TotalRewards)\n",
        "            break\n",
        "\n",
        "        # Update State\n",
        "        State=NewState\n",
        "\n",
        "# Close Environment\n",
        "Env.close()\n",
        "print (\"Score Over Time: \"+str(sum(Rewards)/TotalTestEpisodes))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Episode  0\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | :\u001b[43m \u001b[0m: |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[43m \u001b[0m: : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[43m \u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[43m \u001b[0m| : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[42mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[42m_\u001b[0m| : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[42m_\u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[42m_\u001b[0m: : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[42m_\u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[42m_\u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[42m_\u001b[0m: |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35m\u001b[42mB\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "Score  5\n",
            "Score Over Time: 5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zuNbttdlx60"
      },
      "source": [
        "%%time\n",
        "\"\"\"Training the agent\"\"\"\n",
        "\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.6\n",
        "epsilon = 0.1\n",
        "max_steps = 99\n",
        "total_test_episodes = 100\n",
        "\n",
        "# For plotting metrics\n",
        "all_epochs = []\n",
        "all_penalties = []\n",
        "\n",
        "for i in range(1, 100001):\n",
        "    state = env.reset()\n",
        "\n",
        "    epochs, penalties, reward, = 0, 0, 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample() # Explore action space\n",
        "        else:\n",
        "            action = np.argmax(q_table[state]) # Exploit learned values\n",
        "\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        old_value = q_table[state, action]\n",
        "        next_max = np.max(q_table[next_state])\n",
        "\n",
        "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "        q_table[state, action] = new_value\n",
        "\n",
        "        if reward == -10:  #computing rewards and penalties\n",
        "            penalties += 1\n",
        "\n",
        "        state = next_state\n",
        "        epochs += 1\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        clear_output(wait=True)\n",
        "        print(\"Episode:\",{i})\n",
        "\n",
        "print(\"Training finished.\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNMC5P0OspmZ"
      },
      "source": [
        "#evaluate the agent's performance\n",
        "\n",
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "\n",
        "env.reset()\n",
        "\n",
        "\n",
        "rewards = []\n",
        "\n",
        "for episode in range(total_test_episodes):\n",
        "    state = env.reset()\n",
        "    env.encode(1, 1, 3, 2)\n",
        "\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "    print(\"****************************************************\")\n",
        "    print(\"EPISODE \", episode)\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        #  The AGENT is PLAYING\n",
        "        env.render()\n",
        "        # Take the action (index) that have the maximum expected future reward given that state\n",
        "        action = np.argmax(q_table[state])\n",
        "\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        total_rewards += reward\n",
        "        if done:\n",
        "            rewards.append(total_rewards)\n",
        "            print (\"Score\", total_rewards)\n",
        "            break\n",
        "        state = new_state\n",
        "env.close()\n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_test_episodes))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}